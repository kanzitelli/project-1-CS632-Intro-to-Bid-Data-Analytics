{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop-words\n",
      "  Downloading stop-words-2015.2.23.1.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Running setup.py bdist_wheel for stop-words ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/kanzitdinov/Library/Caches/pip/wheels/22/74/80/77275c2f9f2f1d9841b51e169a38985640a10fbd2711d10791\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2015.2.23.1\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERM --- DOC. FREQ. --- POSTRINGS LISTS\n",
      "\n",
      "forecasts - 1 - ['doc1']\n",
      "home - 4 - ['doc2', 'doc3', 'doc1', 'doc4']\n",
      "increase - 1 - ['doc3']\n",
      "july - 3 - ['doc2', 'doc3', 'doc4']\n",
      "new - 2 - ['doc1', 'doc4']\n",
      "rise - 2 - ['doc2', 'doc4']\n",
      "sales - 4 - ['doc2', 'doc3', 'doc1', 'doc4']\n",
      "top - 1 - ['doc1']\n",
      "____________________________________\n"
     ]
    }
   ],
   "source": [
    "# Project #1\n",
    "# Task: to build a Basic Inverted Index\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# CONSTANTS\n",
    "DOCS = ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt'] # an array with file names where paragraphs are\n",
    "DOCS_TEST = ['test1.txt', 'test2.txt']\n",
    "STOP_WORDS = get_stop_words('en')\n",
    "\n",
    "def collect_all_docs(docs):\n",
    "    # return - { docName: text, ... }\n",
    "    docs_info = {}\n",
    "    for doc_name in docs:\n",
    "        with open(doc_name) as f: indata = f.read()\n",
    "        name = doc_name.split('.')[0]\n",
    "        docs_info[name] = indata.lower()\n",
    "    return docs_info\n",
    "\n",
    "def tokenize_text_of(docs): \n",
    "    # return - [ (term, docID), ... ]\n",
    "    tokenized = []\n",
    "    for key, text in docs.iteritems():\n",
    "        array_of_words = text.split()\n",
    "        for word in array_of_words:\n",
    "            cleanWord = \"\"\n",
    "            for char in word:\n",
    "                if char in '!,.?\\'\":;0123456789': # we don't want whose symbols \n",
    "                    char = \"\"\n",
    "                cleanWord += char\n",
    "            if cleanWord != \"\" and not cleanWord in STOP_WORDS: # checks if it is in STOP_WORDS or not\n",
    "                tokenized.append((cleanWord, key))\n",
    "    return tokenized\n",
    "\n",
    "def build_inverted_index_with(words):\n",
    "    # return - [ ( (term, doc. freq.), [doc_name, ...] ), ... ]\n",
    "    words_len = len(words)\n",
    "    index_info = []\n",
    "    i = 0\n",
    "    while i in range(0, words_len):\n",
    "        term, doc = words[i]\n",
    "        doc_freq = 1\n",
    "        doc_names = [doc]\n",
    "        last = i + 1\n",
    "        for j in range(i + 1, words_len):\n",
    "            term2, doc2 = words[j]\n",
    "            if term == term2:\n",
    "                if not doc2 in doc_names:\n",
    "                    doc_freq += 1\n",
    "                    doc_names.append(doc2)\n",
    "            else:\n",
    "                last = j\n",
    "                break\n",
    "        i = last\n",
    "        index_info.append( ( (term, doc_freq), doc_names ) )\n",
    "    return index_info\n",
    "        \n",
    "docs = collect_all_docs(DOCS)\n",
    "#print docs_info\n",
    "#print('____________________________________')\n",
    "\n",
    "tokenized_text = tokenize_text_of(docs)\n",
    "#print tokenized_text\n",
    "#print('____________________________________')\n",
    "\n",
    "sorted_words = sorted(tokenized_text, key=lambda tup: tup[0])\n",
    "#print sorted_words\n",
    "#print('____________________________________')\n",
    "\n",
    "index_info = build_inverted_index_with(sorted_words)\n",
    "print 'TERM --- DOC. FREQ. --- POSTRINGS LISTS\\n'\n",
    "for info in index_info:\n",
    "    term_doc_freq, postings_lists = info\n",
    "    print '{0} - {1} - {2}'.format(term_doc_freq[0], term_doc_freq[1], postings_lists)\n",
    "print('____________________________________')\n",
    "\n",
    "#print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
